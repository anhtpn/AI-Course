{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression_DNN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZhA9kziWe2ZSpAgnXv+UI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhtpn/AI-Course/blob/main/Regression_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression\n",
        "In this laboratory, we target on DNN architecture and implementation experiences. A model, designed and investigated in a variety of configurations, is implemented to predict car price in the data set."
      ],
      "metadata": {
        "id": "hrCvv6CcaQAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "In this section, we need to download samples of the data set, transform their structures into vectors, and then prepare 3 subsets, including training, validation, and test sets."
      ],
      "metadata": {
        "id": "IrnLhzAxbCOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cLgwh7h-h3V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Make NumPy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Normalization, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "tnvj7Wlm-9xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First import the dataset using *pandas*:"
      ],
      "metadata": {
        "id": "8li5J_ZSE7At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('CarPrice_Assignment.csv')"
      ],
      "metadata": {
        "id": "czcDFjKi_gle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "4gaqFsau_yqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "sUX7edeiitKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.duplicated().all()"
      ],
      "metadata": {
        "id": "MATMA2BypOmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop('car_ID',axis=1)"
      ],
      "metadata": {
        "id": "hEjIfnt5F2sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "K78W9-XEiO6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def car_name(x):\n",
        "    carname  = x.split(' ')[0]\n",
        "    return carname"
      ],
      "metadata": {
        "id": "CJIIaDuHsvOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['CarName']   = dataset['CarName'].apply(car_name)\n",
        "dataset['CarName'].unique()"
      ],
      "metadata": {
        "id": "TRlPUuU2szAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label encode the values in the columns which is categorical"
      ],
      "metadata": {
        "id": "S3C20SLkIbB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quanlitative = [f for f in dataset.columns if dataset.dtypes[f] == 'object']\n",
        "le = LabelEncoder()\n",
        "for col in quanlitative:\n",
        "    dataset[col] = le.fit_transform(dataset[col])"
      ],
      "metadata": {
        "id": "XKUWnwamGvfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataset['price']\n",
        "X = dataset.drop(['price'],axis=1)"
      ],
      "metadata": {
        "id": "x_kelQS2LETE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the data\n",
        "After preprocessing samples, we need to divide into subsets, including\n",
        "\n",
        "Training set: calculate gradients, update weights\n",
        "Validation set: monitor network performance during training phase and trigger termination\n",
        "Test set: evaluate network performance using un-seen samples (samples are never seen by the network yet)\n",
        "To be simple, we split 20% of the training set to obtain the validation set. Please note that samples are collected randomly, not in consecutive block."
      ],
      "metadata": {
        "id": "cpy9aG7LJD-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "yAAWpm3qJWQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "X8Gz0XUYMSPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Normalization layer\n",
        "The Normalization is a clean and simple way to add feature normalization into your model.\n",
        "\n",
        "The first step is to create the layer:"
      ],
      "metadata": {
        "id": "csiP4AKyM28Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalization(axis=-1)"
      ],
      "metadata": {
        "id": "EMpEVw8si0sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, fit the state of the preprocessing layer to the data by calling Normalization.adapt:"
      ],
      "metadata": {
        "id": "peP6A_KyNC63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer.adapt(np.array(X_train))"
      ],
      "metadata": {
        "id": "2OxBVVxBM_Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression using a DNN"
      ],
      "metadata": {
        "id": "-EPCBJ-rP4Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "...\n",
        "def build_and_compile_model():\n",
        "  model = Sequential()\n",
        "  model.add(normalizer)\n",
        "  model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error',\n",
        "                optimizer=Adam(0.1))\n",
        "  return model"
      ],
      "metadata": {
        "id": "D5A-q9c5ivKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model = build_and_compile_model()"
      ],
      "metadata": {
        "id": "xsIIUMbzWYF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model.summary()"
      ],
      "metadata": {
        "id": "xowDopp5WdOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Callbacks: Early Stopping, Model Checkpoint\n",
        "How many epochs should we train the network to obtain a \"good\" performance, 10, 20, 30, etc. ? \\\n",
        "--> Apply Early Stopping to track the performance in validation set during the training phase, and terminate it when the performance stops improving.\n",
        "\n",
        "Moreover, we should save the model when it reach the highest performance in the training phase. Otherwise, we get the latest weights (after the last epoch) that maybe not the best ones."
      ],
      "metadata": {
        "id": "qyfnnaQrlNYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_cb = EarlyStopping(\n",
        "    monitor='val_loss', # track the validation accuracy\n",
        "    patience=3, # val_loss doesn't improve after 3 consecutive epochs, stop!\n",
        "    verbose=1)\n",
        "\n",
        "model_checkpoint_cb = ModelCheckpoint(\n",
        "    'model_checkpoint', \n",
        "    monitor='val_loss', # track val_loss\n",
        "    verbose=1, \n",
        "    save_best_only=True,    # overwrite saved model, keep only the best one\n",
        "    )"
      ],
      "metadata": {
        "id": "p4fjccLWhJ8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = dnn_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    verbose=1, epochs=100, batch_size = 2,\n",
        "    callbacks=[early_stopping_cb, model_checkpoint_cb])"
      ],
      "metadata": {
        "id": "yh58DIk-WiJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error [price]')\n",
        "  plt.legend()\n",
        "  plt.grid(True)"
      ],
      "metadata": {
        "id": "PYeeQWMEacDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "UKSxmAetWvJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation phase\n",
        "Now, evaluate the pre-trained network in test set."
      ],
      "metadata": {
        "id": "iYAWbgFLlW5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model.evaluate(X_test, y_test, verbose=0)"
      ],
      "metadata": {
        "id": "rTEvlqCXcFRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions\n",
        "You can now make predictions with the dnn_model on the test set using Keras Model.predict and review the loss"
      ],
      "metadata": {
        "id": "6wDiALoLlvqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = dnn_model.predict(X_test).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.xlabel('True Values [price]')\n",
        "plt.ylabel('Predictions [price]')\n"
      ],
      "metadata": {
        "id": "tamF2X3Ek666"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NVhtnNaFmB9j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}